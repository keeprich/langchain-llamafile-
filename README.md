# ğŸ§  Llamafile LangChain Wrapper

Use local GGUF models with LangChain via `llamafile`. No server, no API keys â€” just fast, auditable inference.

## Features
- ğŸ”Œ Plug-and-play with LangChain
- ğŸ§  Supports Mistral, LLaMA, and other GGUF models
- âš¡ GPU offloading with `-ngl`
- ğŸ” Optional streaming output
- ğŸ§¼ Clean prompt handling and logging

## Install

```bash
pip install llamafile-langchain

# 🧠 Llamafile LangChain Wrapper

Use local GGUF models with LangChain via `llamafile`. No server, no API keys — just fast, auditable inference.

## Features
- 🔌 Plug-and-play with LangChain
- 🧠 Supports Mistral, LLaMA, and other GGUF models
- ⚡ GPU offloading with `-ngl`
- 🔁 Optional streaming output
- 🧼 Clean prompt handling and logging

## Install

```bash
pip install llamafile-langchain
